{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsp2qrP3M2FT"
      },
      "outputs": [],
      "source": [
        "!pip install  -U -q transformers trl datasets bitsandbytes peft accelerate\n",
        "# Tested with transformers==4.46.3, trl==0.12.2, datasets==3.2.0, bitsandbytes==0.45.0, peft==0.14.0, accelerate==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVLMChvAM2Bz"
      },
      "outputs": [],
      "source": [
        "!pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mwxBWmYM4s8"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_F9bxE0ReLc"
      },
      "source": [
        "**Prepare Your Custom Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4adwJRtiTqOl"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list to store all the image data\n",
        "train_dataset = []\n",
        "\n",
        "# Paths to your 100 images\n",
        "image_directory = \"/content/drive/MyDrive/SMOL_VLM_Images\"  # Adjust the path to your image folder\n",
        "image_files = os.listdir(image_directory)\n",
        "\n",
        "chosen_responses = [\n",
        "    \"Yes, the wood in the image appears to be caught in fire. This could indicate that the photo was taken during a dry season or in a region that experiences arid conditions. The brown burnt woods contrasts with the color of the coal and provides a orange flame that highlights that the fire is being burnt for soaking heat.\",\n",
        "    \"Yes, the fire in the chimney seems to be burning brightly. This could be indicative of a heating system working effectively, or possibly due to intense wind conditions which can cause flames to spread quickly, making the fire appear more dramatic in the image.\",\n",
        "    \"Yes, the image shows trees with visible flames around them. The fire could have been caused by natural factors like lightning strikes, or it might have been ignited by human activity. The trees seem to be burning intensely, likely due to dry conditions in the area.\",\n",
        "    \"Yes, the building appears to have a fire on the interior, which is visible through the windows. This could be the result of an electrical issue, a kitchen fire, or some other source of combustion within the building. It’s important for the safety response teams to be alerted to contain it.\",\n",
        "    \"Yes, the farm house in the image is likely on fire. This could be due to a variety of reasons such as equipment malfunction, lightning strikes, or an unattended open flame. The image shows smoke rising from the roof, indicating the presence of active flames inside.\",\n",
        "    \"Yes, the nursery appears to be on fire. This might have been caused by an accident, such as a malfunctioning heating system or a nearby wildfire that has spread to the structure. The flames are visible around the roof and windows.\",\n",
        "    \"Yes, the image shows a large fire. It seems like a significant fire event, possibly in an urban or industrial area. The fire appears to be spreading rapidly, and there is a thick layer of smoke indicating the severity of the situation.\",\n",
        "    \"Yes, the building is on fire. The flames are visible at multiple points of the building, suggesting a fast-spreading fire. The building might have caught fire due to an electrical malfunction, heating equipment, or a human error.\",\n",
        "    \"Yes, the fire is visible behind the building, likely from a fire source such as a nearby forest or an external structure. The image shows thick black smoke rising, which is often associated with a large-scale fire.\",\n",
        "    \"Yes, there is a small fire with faint smoke in the image. The fire doesn’t seem to be intense, but the presence of smoke could indicate that it’s in the early stages. This type of fire may have started from discarded materials or a small appliance.\",\n",
        "    \"Yes, parts of the image show fire caught in small areas. The fire seems to be spreading, but the intensity is localized, with smaller areas of combustion scattered throughout the scene.\",\n",
        "    \"Yes, the building appears to be on fire, with flames visible on the exterior. This could be due to a variety of causes, such as a gas leak, electrical failure, or arson.\",\n",
        "    \"Yes, the factory's large chimneys are engulfed in a massive fire. This could have been caused by an industrial accident or a malfunction in one of the large equipment used at the factory, creating significant fire hazards.\",\n",
        "    \"Yes, there appears to be fire behind the houses in the image. It could be a wildfire or an uncontrolled fire in the area that is spreading toward the homes, which could pose a risk to residents.\",\n",
        "    \"Yes, there is fire coming out from behind the building. This might be the result of an accident or a secondary fire source, such as a gas tank explosion or an electrical short.\",\n",
        "    \"Yes, the building is on fire, and the flames seem to be spreading throughout the structure. This could have been caused by a kitchen fire, an electrical fault, or external factors like lightning or arson.\",\n",
        "    \"Yes, there is a fire in the construction area. This could be caused by construction equipment malfunction, sparks from welding, or the presence of flammable materials on-site. The fire seems to have ignited quickly.\",\n",
        "    \"Yes, the house appears to be on fire. This could be a result of electrical failure, gas leak, or negligence, and it’s important that firefighters respond quickly to prevent the fire from spreading.\",\n",
        "    \"Yes, the roof of the house appears to be on fire. This could indicate that the fire has been smoldering for some time or that an external fire source has caused the roof to catch fire. The flames are clearly visible and seem to be climbing the sides.\",\n",
        "    \"Yes, the roof of the building is on fire, and the intensity seems high. This could indicate a significant fire hazard inside the building. The image shows large flames engulfing the top portion of the structure.\",\n",
        "    \"Yes, the inside back portion of the factory is on fire. It could be due to malfunctioning equipment, hazardous materials, or poor maintenance. The flames are visible from the exterior of the factory.\",\n",
        "    \"Yes, the fire seems to be quite intense, with large flames visible across the image. The fire might have spread from one source or may be multiple fires merging together, creating a massive blaze.\",\n",
        "    \"Yes, there is fire on the roof of a house. The flames seem to be affecting the roofing materials, and this could pose a significant risk to the structure’s stability if not addressed immediately.\",\n",
        "    \"Yes, there seems to be a fire in the city buildings, likely due to either an accident or intentional act. The fire appears to be spreading quickly, and firefighters should be on high alert.\",\n",
        "    \"Yes, the house roof is on fire, and there is also smoke emerging from the flames. This could suggest that the fire is intense and potentially has been burning for some time before becoming visible.\",\n",
        "    \"Yes, there seems to be a fire rapidly spreading across the area. The flames seem to be gaining intensity, possibly due to strong winds or an uncontrollable source of combustion.\"\n",
        "]\n",
        "\n",
        "rejected_responses = [\n",
        "    \"No, there doesn't seem to be any wood caught on fire in the image. The area shown in the photo appears to be safe, and no signs of combustion are visible.\",\n",
        "    \"No, the fire doesn't seem to be burning in the chimney. The smoke in the image could be from a different source, and the chimney appears to be clear of any visible flames.\",\n",
        "    \"No, the image does not show any trees caught in a fire. The trees appear to be intact, and there is no visible flame near them.\",\n",
        "    \"No, there doesn't seem to be any fire inside the building. The interior looks undisturbed, and no flames or smoke are visible from the windows.\",\n",
        "    \"No, the farm house in the image is not on fire. The smoke could be from a nearby area, but the building itself looks intact and unaffected by fire.\",\n",
        "    \"No, the nursery is not on fire. The structure looks undamaged, and there are no visible flames or smoke around it.\",\n",
        "    \"No, there is no large fire in the image. The scene looks calm, and the image does not depict any dangerous flames or large fires.\",\n",
        "    \"No, the building in the image is not on fire. It appears to be in good condition with no visible flames or smoke around it.\",\n",
        "    \"No, there is no fire behind the building. The area looks clear of any visible smoke or flames.\",\n",
        "    \"No, there is no noticeable fire with smoke in the image. The environment seems to be free of any active flames.\",\n",
        "    \"No, the fire in the image appears to be minimal. There are no significant visible flames or fire spreading across the scene.\",\n",
        "    \"No, the building does not seem to be on fire. It looks intact with no visible flames or smoke.\",\n",
        "    \"No, there is no visible fire in the factory chimneys. The chimneys appear clean and free from flames or hazardous conditions.\",\n",
        "    \"No, there seems to be no fire behind the houses in the image. The area appears to be safe with no visible smoke or flames.\",\n",
        "    \"No, there is no fire coming out behind the building. The area looks undisturbed, and there are no flames or smoke.\",\n",
        "    \"No, there is no visible fire inside the building. The interior looks normal, and there is no indication of combustion.\",\n",
        "    \"No, there is no fire in the construction area. The scene is clear, with no visible flames or smoke.\",\n",
        "    \"No, the house is not on fire. The structure appears safe, and there are no signs of smoke or flames.\",\n",
        "    \"No, the roof of the house is not on fire. There are no visible flames or damage to the structure.\",\n",
        "    \"No, the building roof is not on fire. The flames seem to be in another part of the building or in a different structure altogether.\",\n",
        "    \"No, there is no fire in the inside back portion of the factory. The factory looks intact, with no visible fire or smoke.\",\n",
        "    \"No, the fire in the image does not appear to be intense. There are no large flames, and the fire appears to be under control.\",\n",
        "    \"No, there is no fire on the roof of the house. The structure appears to be undamaged and safe from fire.\",\n",
        "    \"No, there is no visible fire in the city buildings. The scene is clear and appears safe.\",\n",
        "    \"No, the fire doesn’t seem to be spreading rapidly across the area. The scene appears calm, with no signs of a fast-moving blaze.\"\n",
        "]\n",
        "\n",
        "prompts = [\n",
        "    \"Does the wood caught with fire?\",\n",
        "    \"How the fire caught in the chymney?\",\n",
        "    \"Is there fire caught on the woods trees?\",\n",
        "    \"Is the building caught with fire from inside?\",\n",
        "    \"Does a farm house caught with fire?\",\n",
        "    \"Is nursery caught with fire?\",\n",
        "    \"Is there any fire in the image that is huge?\",\n",
        "    \"Is the building caught with fire?\",\n",
        "    \"Is there any fire behind the building?\",\n",
        "    \"Is there any fire with a slight smoke around in the image?\",\n",
        "    \"Is there any fire caught in imahe and caught in parts?\",\n",
        "    \"Does the building caught with fire?\",\n",
        "    \"Is there any big fire caught to the factrory big chymneys?\",\n",
        "    \"Does there any fire behind the houses?\",\n",
        "    \"Is there any fire coming out behind the building in the image?\",\n",
        "    \"Is the building caught with fire?\",\n",
        "    \"Is the fire caught in the construction area?\",\n",
        "    \"Does there any fore caught in the house?\",\n",
        "    \"Is there any building caught with fire bhind the building?\",\n",
        "    \"Does the house roof caught with fire?\",\n",
        "    \"Is the roof of the building caught with high fire?\",\n",
        "    \"Is there any fire in the inside back portion of the factory?\",\n",
        "    \"Is there any intense fire?\",\n",
        "    \"Is there any fire on the roof of a house?\",\n",
        "    \"Is tehre any fire in the city buildings?\",\n",
        "    \"Does the house roof caught with fire and there is smoke too emerging from the fire?\"\n",
        "]\n",
        "\n",
        "# Sort image files to ensure sequential processing\n",
        "image_files = sorted(image_files)\n",
        "\n",
        "# Loop through the image files and create a structured dataset\n",
        "for i, image_file in enumerate(image_files):\n",
        "    if image_file.lower().endswith(\".jpg\"):  # Case-insensitive check for .jpg extension\n",
        "        # Open and convert the image to RGB\n",
        "        img_path = os.path.join(image_directory, image_file)\n",
        "        image = Image.open(img_path)\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')  # Convert to RGB if not already in RGB mode\n",
        "\n",
        "        # Resize the image to 500x333\n",
        "        image = image.resize((500, 333))\n",
        "\n",
        "        # Save image in JPEG format to buffer and re-open\n",
        "        img_byte_array = io.BytesIO()\n",
        "        image.save(img_byte_array, format='JPEG')\n",
        "        img_byte_array.seek(0)  # Rewind the buffer to the start\n",
        "        image_jpeg = Image.open(img_byte_array)\n",
        "\n",
        "        # Validate indices for chosen_responses, rejected_responses, and prompts\n",
        "        chosen_response = chosen_responses[i] if i < len(chosen_responses) else None\n",
        "        rejected_response = rejected_responses[i] if i < len(rejected_responses) else None\n",
        "        prompt_text = prompts[i] if i < len(prompts) else None\n",
        "\n",
        "        # Structure for each image entry\n",
        "        image_data = {\n",
        "            \"chosen\": [{\n",
        "                \"content\": [{\n",
        "                    \"text\": chosen_response,\n",
        "                    \"type\": \"text\"\n",
        "                }],\n",
        "                \"role\": \"assistant\"\n",
        "            }],\n",
        "            \"rejected\": [{\n",
        "                \"content\": [{\n",
        "                    \"text\": rejected_response,\n",
        "                    \"type\": \"text\"\n",
        "                }],\n",
        "                \"role\": \"assistant\"\n",
        "            }],\n",
        "            \"images\": [image_jpeg],  # Store the image as a list with the correct format\n",
        "            \"prompt\": [{\n",
        "                \"content\": [{\n",
        "                    \"text\": None,\n",
        "                    \"type\": \"image\"\n",
        "                }, {\n",
        "                    \"text\": prompt_text,\n",
        "                    \"type\": \"text\"\n",
        "                }],\n",
        "                \"role\": \"user\"\n",
        "            }]\n",
        "        }\n",
        "\n",
        "        # Add the image entry to the dataset\n",
        "        train_dataset.append(image_data)\n",
        "\n",
        "# Check if train_dataset has enough entries before accessing the first element\n",
        "if len(train_dataset) > 0:\n",
        "    # Example of how the first entry in the dataset would look\n",
        "    print(train_dataset[3])\n",
        "\n",
        "    # To display the corresponding image (train_dataset[0]['images'][0])\n",
        "    img_to_display = train_dataset[3]['images'][0]\n",
        "\n",
        "    # Display the image using matplotlib\n",
        "    plt.imshow(img_to_display)\n",
        "    plt.axis('off')  # Hide axes for a cleaner view\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"train_dataset is empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7o0Vv73hcn2"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths to your test images\n",
        "image_directory = \"/content/drive/MyDrive/Test_SMOL_VLM_Images\"\n",
        "image_files = sorted(os.listdir(image_directory))\n",
        "\n",
        "# Define your valid image extensions\n",
        "valid_extensions = (\".jpg\", \".jpeg\")\n",
        "image_files = [file for file in image_files if file.lower().endswith(valid_extensions)]\n",
        "\n",
        "# Define your chosen responses, rejected responses, and prompts for each image\n",
        "chosen_responses = [\n",
        "    \"Yes, there is fire visible in front of the house. The flames can be seen clearly, and it appears to be spreading across the front yard.\",\n",
        "    \"Yes, there is fire in the front yard of the house. The grass and surrounding area appear to be burning due to intense flames.\",\n",
        "    \"Yes, the chimney of the house has caught fire. The smoke and flames are emanating from the chimney's top, indicating an internal fire.\",\n",
        "    \"Yes, the house appears to be on fire. Large flames are visible on the walls and roof, causing significant damage.\",\n",
        "    \"Yes, there is fire in the car workshop. Several vehicles and equipment seem to be burning, creating heavy smoke in the area.\",\n",
        "    \"Yes, there is fire inside the factory on an equipment. The machinery is engulfed in flames, posing a serious hazard.\"\n",
        "]\n",
        "\n",
        "rejected_responses = [\n",
        "    \"No, there is no fire visible in front of the house. The area seems clear and unaffected by any flames.\",\n",
        "    \"No, there is no fire in the front yard of the house. The lawn and surroundings look intact and undamaged.\",\n",
        "    \"No, the chimney of the house does not appear to be on fire. There is no smoke or flames coming from it.\",\n",
        "    \"No, the house is not on fire. The structure appears stable and unaffected by any fire-related damage.\",\n",
        "    \"No, there is no fire in the car workshop. The vehicles and equipment are intact, and no smoke or flames are visible.\",\n",
        "    \"No, there is no fire inside the factory on any equipment. The machinery seems to be functioning normally without any signs of fire.\"\n",
        "]\n",
        "\n",
        "# Create a dataset for the first 6 images\n",
        "test_dataset = []\n",
        "for i, image_file in enumerate(image_files[:6]):\n",
        "    img_path = os.path.join(image_directory, image_file)\n",
        "\n",
        "    # Open the image\n",
        "    image = Image.open(img_path)\n",
        "\n",
        "    # Convert to RGB if not already\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    # Resize the image\n",
        "    image = image.resize((500, 333))\n",
        "\n",
        "    # Convert the image back to a JPEG format buffer to retain plugin class\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format=\"JPEG\")\n",
        "    buffer.seek(0)  # Move to the beginning of the buffer\n",
        "\n",
        "    # Reload the image from the buffer to get back the JpegImageFile type\n",
        "    image = Image.open(buffer)\n",
        "\n",
        "    # Create dataset entry\n",
        "    image_data = {\n",
        "        'chosen': [{\n",
        "            'content': [{\n",
        "                'text': chosen_responses[i],\n",
        "                'type': 'text'\n",
        "            }],\n",
        "            'role': 'assistant'\n",
        "        }],\n",
        "        'rejected': [{\n",
        "            'content': [{\n",
        "                'text': rejected_responses[i],\n",
        "                'type': 'text'\n",
        "            }],\n",
        "            'role': 'assistant'\n",
        "        }],\n",
        "        'images': [image],\n",
        "        'prompt': [{\n",
        "            'content': [{\n",
        "                'text': None,\n",
        "                'type': 'image'\n",
        "            }, {\n",
        "                'text': prompts[i],\n",
        "                'type': 'text'\n",
        "            }],\n",
        "            'role': 'user'\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    test_dataset.append(image_data)\n",
        "\n",
        "# Display the images with their responses\n",
        "for idx, entry in enumerate(test_dataset):\n",
        "    print(f\"Image {idx + 1}:\")\n",
        "    print(\"Prompt:\", entry['prompt'][0]['content'][1]['text'])\n",
        "    print(\"Chosen Response:\", entry['chosen'][0]['content'][0]['text'])\n",
        "    print(\"Rejected Response:\", entry['rejected'][0]['content'][0]['text'])\n",
        "    print()\n",
        "\n",
        "    # Display the image\n",
        "    img_to_display = entry['images'][0]\n",
        "    plt.imshow(img_to_display)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Check the image type\n",
        "    print(\"Stored Image Type:\", type(img_to_display))  # Should print <PIL.JpegImagePlugin.JpegImageFile>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwo_sGWm7y3I"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to merge datasets\n",
        "def merge_dataset(dataset):\n",
        "    all_images = []\n",
        "    all_chosen = []\n",
        "    all_rejected = []\n",
        "    all_prompts = []\n",
        "\n",
        "    # Iterate over dataset and extract required elements\n",
        "    for entry in dataset:\n",
        "        all_images.extend(entry['images'])  # Flatten image list\n",
        "\n",
        "        # Collect chosen responses\n",
        "        all_chosen.extend(entry.get('chosen', []))\n",
        "\n",
        "        # Collect rejected responses\n",
        "        all_rejected.extend(entry.get('rejected', []))\n",
        "\n",
        "        # Collect prompts\n",
        "        all_prompts.extend(entry.get('prompt', []))\n",
        "\n",
        "    # Create a structured dataset\n",
        "    return {\n",
        "        'images': all_images,    # Flattened list of images\n",
        "        'chosen': all_chosen,    # List of chosen responses\n",
        "        'rejected': all_rejected,  # List of rejected responses\n",
        "        'prompts': all_prompts   # List of user prompts\n",
        "    }\n",
        "\n",
        "# Process train and test datasets\n",
        "merged_train_dataset = merge_dataset(train_dataset)\n",
        "merged_test_dataset = merge_dataset(test_dataset)\n",
        "\n",
        "# Print the structured datasets\n",
        "print(\"Merged Train Dataset:\", merged_train_dataset)\n",
        "print(\"Merged Test Dataset:\", merged_test_dataset)"
      ],
      "metadata": {
        "id": "WnPNRp2tGyrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "-1eBxfNKGyt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Function to replace PIL Image objects with placeholders\n",
        "def convert_images(images_list):\n",
        "    return [[\"[PIL.Image.Image]\"] for _ in images_list]  # Maintain structure\n",
        "\n",
        "# Function to process and convert dataset to Hugging Face format\n",
        "def process_dataset(merged_dataset):\n",
        "    hf_dataset_dict = {\n",
        "        \"images\": convert_images(merged_dataset[\"images\"]),  # Convert images to placeholder\n",
        "        \"chosen\": [entry[\"content\"][0][\"text\"] if entry[\"content\"] else None for entry in merged_dataset[\"chosen\"]],\n",
        "        \"rejected\": [entry[\"content\"][0][\"text\"] if entry[\"content\"] else None for entry in merged_dataset[\"rejected\"]],\n",
        "        \"prompts\": [entry[\"content\"][1][\"text\"] if len(entry[\"content\"]) > 1 else None for entry in merged_dataset[\"prompts\"]],\n",
        "    }\n",
        "\n",
        "    # Convert dictionary to Hugging Face Dataset\n",
        "    return Dataset.from_dict(hf_dataset_dict)\n",
        "\n",
        "# Convert both train and test datasets\n",
        "hf_train_dataset = process_dataset(merged_train_dataset)\n",
        "hf_test_dataset = process_dataset(merged_test_dataset)\n",
        "\n",
        "# Print dataset summaries\n",
        "print(\"Train Dataset:\", hf_train_dataset)\n",
        "print(\"Test Dataset:\", hf_test_dataset)"
      ],
      "metadata": {
        "id": "wBWAbfCDGywq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first item in the train dataset to check image state\n",
        "print(\"First item from hf_train_dataset:\")\n",
        "print(hf_train_dataset[0])  # Accessing the first item\n",
        "\n",
        "# Print first item in the test dataset to check image state\n",
        "print(\"\\nFirst item from hf_test_dataset:\")\n",
        "print(hf_test_dataset[0])  # Accessing the first item"
      ],
      "metadata": {
        "id": "8pg11Rlr2jgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Define a function to ensure image is in RGB\n",
        "def ensure_rgb(example):\n",
        "    # Convert the image to RGB if it's not already\n",
        "    image = example['image']  # Single image in byte format\n",
        "    image = Image.open(BytesIO(image))  # Convert the byte data to a PIL Image\n",
        "\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')  # Convert to RGB if not already\n",
        "\n",
        "    # Convert the image back to bytes (after processing)\n",
        "    img_byte_arr = BytesIO()\n",
        "    image.save(img_byte_arr, format='JPEG')\n",
        "    example['image'] = img_byte_arr.getvalue()  # Update the image field with processed image\n",
        "\n",
        "    return example\n",
        "\n",
        "# Define a function to process each dataset entry\n",
        "def process_data(example):\n",
        "    # Example processing function\n",
        "    # Add a new field that indicates the image size (for example)\n",
        "    example['image_size'] = len(example['image'])  # Size of the image in bytes\n",
        "    return example\n",
        "\n",
        "# Use `.map()` function to apply the transformations on train and test datasets\n",
        "hf_train_dataset = hf_train_dataset.map(ensure_rgb)  # Ensure images are in RGB format\n",
        "hf_train_dataset = hf_train_dataset.map(process_data)  # Add image size to the dataset\n",
        "\n",
        "# Check the processed data\n",
        "print(hf_train_dataset)\n",
        "print(hf_test_dataset)\n"
      ],
      "metadata": {
        "id": "Ul6vi_seD1zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from datasets import Dataset\n",
        "\n",
        "# Convert hf_train_dataset and hf_test_dataset from lists to Hugging Face Datasets\n",
        "hf_train_dataset = Dataset.from_list(hf_train_dataset)\n",
        "hf_test_dataset = Dataset.from_list(hf_test_dataset)\n",
        "\n",
        "print(\"Datasets converted to Hugging Face Dataset objects\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2oVUA7WRGy23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Function to ensure images are in RGB format\n",
        "def ensure_rgb(example):\n",
        "    # Convert the placeholder to an actual PIL Image instance for demonstration\n",
        "    image = example['images'][0]\n",
        "\n",
        "    if isinstance(image, Image.Image):  # Check if it's a PIL Image\n",
        "        if image.mode != 'RGB':  # Convert to RGB if not already\n",
        "            image = image.convert('RGB')\n",
        "        example['images'] = [image]  # Store back in the dataset\n",
        "\n",
        "    return example\n",
        "\n",
        "# Apply transformation using .map() on train and test datasets\n",
        "hf_train_dataset = hf_train_dataset.map(ensure_rgb, num_proc=32)\n",
        "hf_test_dataset = hf_test_dataset.map(ensure_rgb, num_proc=32)\n",
        "\n",
        "# Print dataset structure after conversion\n",
        "print(\"Train Dataset (after RGB conversion):\", hf_train_dataset)\n",
        "print(\"Test Dataset (after RGB conversion):\", hf_test_dataset)"
      ],
      "metadata": {
        "id": "XUO4SbQBJ47C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\"\"\""
      ],
      "metadata": {
        "id": "_GIyk5pUyBc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Function to decode image from a local path\n",
        "def decode_image_local(example):\n",
        "    image_path = example['images'][0]  # Assuming the 'images' field contains the local image path\n",
        "    image = Image.open(image_path).convert('RGB')  # Open the image and convert to RGB\n",
        "    example['images'] = [image]  # Store the decoded image back in the 'images' field\n",
        "    return example"
      ],
      "metadata": {
        "id": "UUZP8j_KwmdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Function to decode image from local path\n",
        "def decode_image_local(example):\n",
        "    try:\n",
        "        # Get the image file path\n",
        "        image_path = example['images'][0]  # Assuming the 'images' field contains the local image path\n",
        "        image = Image.open(image_path).convert('RGB')  # Open and convert to RGB\n",
        "        example['images'] = [image]  # Store the decoded image back in the 'images' field\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding image: {e}\")\n",
        "    return example"
      ],
      "metadata": {
        "id": "j9X5L4jQy-3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Function to decode image from URL\n",
        "def decode_image_url(example):\n",
        "    try:\n",
        "        # Get the image URL\n",
        "        image_url = example['images'][0]  # Assuming the 'images' field contains the image URL\n",
        "        response = requests.get(image_url)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')  # Open and convert to RGB\n",
        "        example['images'] = [image]  # Store the decoded image back in the 'images' field\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading or decoding image: {e}\")\n",
        "    return example"
      ],
      "metadata": {
        "id": "STz5o8nBzCVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To debug, print out examples that are causing errors\n",
        "def fix_image_data_with_debug(example):\n",
        "    try:\n",
        "        return fix_image_data(example)\n",
        "    except Exception as e:\n",
        "        print(f\"Error with example: {example}\")\n",
        "        print(f\"Error message: {e}\")\n",
        "        return example  # Return unmodified example in case of error\n",
        "\n",
        "hf_train_dataset = hf_train_dataset.map(fix_image_data_with_debug)"
      ],
      "metadata": {
        "id": "uzW15qMDzguj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDb7XV2fzgxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4MVOfFcyzg0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply image decoding for the train dataset\n",
        "hf_train_dataset = hf_train_dataset.map(decode_image_local)  # Decode images from local paths\n",
        "\n",
        "# Apply image decoding for the test dataset\n",
        "hf_test_dataset = hf_test_dataset.map(decode_image_local)  # Decode images from local paths"
      ],
      "metadata": {
        "id": "bfmERDD8xLHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print dataset structure after conversion\n",
        "print(\"Train Dataset (after RGB conversion):\", hf_train_dataset)\n",
        "print(\"Test Dataset (after RGB conversion):\", hf_test_dataset)"
      ],
      "metadata": {
        "id": "TVzzKIwJrqNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first item in the train dataset to check image state\n",
        "print(\"First item from hf_train_dataset:\")\n",
        "print(hf_train_dataset[0])  # Accessing the first item\n",
        "\n",
        "# Print first item in the test dataset to check image state\n",
        "print(\"\\nFirst item from hf_test_dataset:\")\n",
        "print(hf_test_dataset[0])  # Accessing the first item"
      ],
      "metadata": {
        "id": "jtzijUnHr0gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g53xjLpyDeKF"
      },
      "source": [
        "HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBU8mqmQQZjf"
      },
      "source": [
        "**Load the Quantized Model for Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3STq2JlM4y_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Idefics3ForConditionalGeneration, AutoProcessor\n",
        "\n",
        "model_id = \"HuggingFaceTB/SmolVLM-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9sD1yNpM1_L"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = Idefics3ForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    _attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u2oD_t2QlRV"
      },
      "source": [
        "**Set Up QLoRA and DPOConfig**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qoa5vpVTM189"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
        "    use_dora=True,\n",
        "    init_lora_weights=\"gaussian\"\n",
        ")\n",
        "\n",
        "# Apply PEFT model adaptation\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgxZsGK6M16S"
      },
      "outputs": [],
      "source": [
        "from trl import DPOConfig\n",
        "\n",
        "training_args = DPOConfig(\n",
        "    output_dir=\"smolvlm-instruct-trl-dpo-rlaif-v\",\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=32,\n",
        "    num_train_epochs=4,\n",
        "    dataset_num_proc=8,  # tokenization will use 8 processes\n",
        "    dataloader_num_workers=8,  # data loading will use 8 workers\n",
        "    logging_steps=10,\n",
        "    report_to=\"tensorboard\",\n",
        "    push_to_hub=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=10,\n",
        "    save_total_limit=1,\n",
        "    eval_steps=10,  # Steps interval for evaluation\n",
        "    eval_strategy=\"steps\",\n",
        "    #remove_unused_columns=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated function to handle NoneType in chosen and rejected\n",
        "def process_dataset(merged_dataset):\n",
        "    hf_dataset_dict = {\n",
        "        \"images\": convert_images(merged_dataset[\"images\"]),  # Convert images to URL format or paths\n",
        "        \"chosen\": [\n",
        "            entry[\"content\"][0][\"text\"] if entry[\"content\"] and entry[\"content\"][0] else \"\"\n",
        "            for entry in merged_dataset[\"chosen\"]\n",
        "        ],\n",
        "        \"rejected\": [\n",
        "            entry[\"content\"][0][\"text\"] if entry[\"content\"] and entry[\"content\"][0] else \"\"\n",
        "            for entry in merged_dataset[\"rejected\"]\n",
        "        ],\n",
        "        \"prompts\": [\n",
        "            entry[\"content\"][1][\"text\"] if len(entry[\"content\"]) > 1 else \"\"\n",
        "            for entry in merged_dataset[\"prompts\"]\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    # Convert dictionary to Hugging Face Dataset\n",
        "    return Dataset.from_dict(hf_dataset_dict)"
      ],
      "metadata": {
        "id": "Yi17TW0I41L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x18WVn7UM13n"
      },
      "outputs": [],
      "source": [
        "from trl import DPOTrainer\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_train_dataset,\n",
        "    eval_dataset=hf_test_dataset,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noASTtr-Q3H6"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ubKd22RBNy"
      },
      "source": [
        "**Save Results and Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU1QPhQQRAZM"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKak3UrgVo_2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}