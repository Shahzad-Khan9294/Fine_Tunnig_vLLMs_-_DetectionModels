{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFvch5049u6M"
      },
      "source": [
        "# Train NanoDet with custom dataset\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SonySemiconductorSolutions/aitrios-rpi-tutorials-ai-model-training/blob/main/notebooks/nanodet-ppe/custom_nanodet.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Training NanoDet model to detect Personal Protection Equipment (PPE) using open source dataset.\n",
        "\n",
        "Nanodet training based on https://github.com/RangiLyu/nanodet/tree/main\n",
        "\n",
        "Tutorial includes:\n",
        "- Dataset setup\n",
        "- Nanodet model setup\n",
        "- Training\n",
        "- Quantization using [Model Compression Toolkit - MCT](https://github.com/sony/model_optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ueNUElfCPc3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade pip"
      ],
      "metadata": {
        "id": "QHHfZVK5tqxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9npotka_TOY"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==5.29.1 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxQzS2JR_gO7"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxEn0kWJ_TRB"
      },
      "outputs": [],
      "source": [
        "#!pip install numpy==1.26.4 --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4Cyr58_TTf"
      },
      "outputs": [],
      "source": [
        "!pip install \"ml-dtypes>=0.4.0,<0.5.0\" --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NaA6XCI_TXA"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.6.0 torchaudio==2.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8DSlPj_Foxc"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall ml_dtypes==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md-RBm-H9u6R"
      },
      "outputs": [],
      "source": [
        "!pip install -q --no-cache-dir --no-deps torch==2.0.0 torchvision==0.15.1 \"tensorflow>=2.14.0,<2.18.0\" pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN4jlrb2Fyu8"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.3 --force-reinstall --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lft6ODo_Gvdz"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.4 --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DQdpWvcHUPr"
      },
      "outputs": [],
      "source": [
        "# Install the correct torch+cuda (matching Colabâ€™s config)\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "print(\"CUDA memory cleared.\")"
      ],
      "metadata": {
        "id": "7W0LUnW0Bx51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCcVYApnE-na"
      },
      "outputs": [],
      "source": [
        "# In Colab:\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5joYJvQ9u6S"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "\n",
        "assert '2.17.1' in tf.__version__\n",
        "assert '2.0' in torch.__version__\n",
        "\n",
        "shm_stats = shutil.disk_usage('/dev/shm')\n",
        "shm_in_gb = shm_stats.total / (1024 ** 3)\n",
        "print(f\"shm memory: {shm_in_gb:.2f}GB\")\n",
        "\n",
        "print(f'Is cuda available: {torch.cuda.is_available()}')\n",
        "assert shm_in_gb >= 12 or torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3uYCYO39u6T"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWWa58xU9u6U"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Known errors:\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
        "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
        "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
        "\"\"\"\n",
        "NANODET_COMMIT = 'be9b4a9'\n",
        "!rm -rf nanodet\n",
        "!git clone https://github.com/RangiLyu/nanodet.git\n",
        "!touch nanodet/nanodet/model/__init__.py\n",
        "#!cd nanodet && git checkout {NANODET_COMMIT} && pip install -q --no-cache-dir -r /content/nanodet/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1RIz-xe9u6V"
      },
      "source": [
        "# Dataset\n",
        "- go to https://universe.roboflow.com/ai-camp-safety-equipment-detection/ppe-detection-using-cv/dataset/3 and click `\"Download Dataset\"`\n",
        "- choose format `\"COCO\"` and `\"show download code\"` and `\"continue\"`\n",
        "- choose `\"Terminal\"` and copy the command `\"curl...\"` and paste the command in the cell below.\n",
        "- add `\"!\"` in the beginning of the command and replace `\"\\&gt;\"` with `\">\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lugfGdG9u6W"
      },
      "outputs": [],
      "source": [
        "# Add below your download code from Roboflow, it should look like the following, with your unique roboflow dataset url:\n",
        "# Example (with \"!\" added in the beginning of the command and replaced \"&gt;\" with \">\". Also added \"-q\" for less output):\n",
        "# !curl -L \"https://universe.roboflow.com/ds/<unique-dataset-url>\" > roboflow.zip; unzip -q roboflow.zip; rm roboflow.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Path to your dataset folder (adjust if your folder name is different)\n",
        "DATASET_PATH = '/content/drive/MyDrive/dataset_blob'\n",
        "\n",
        "# Just verifying that annotation files exist in each split\n",
        "assert Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists(), \"Train annotations missing\"\n",
        "assert Path(f'{DATASET_PATH}/valid/_annotations.coco.json').exists(), \"Valid annotations missing\"\n",
        "assert Path(f'{DATASET_PATH}/test/_annotations.coco.json').exists(), \"Test annotations missing\"\n",
        "\n",
        "print(\"Custom dataset structure looks good for NanoDet training.\")"
      ],
      "metadata": {
        "id": "m_ZMRHx1DOGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKQM2hnt9u6W"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Move test/train/valid to dataset folder\n",
        "from pathlib import Path\n",
        "DATASET_PATH = 'dataset/PPE_Detection_Using_CV.v3i.coco'\n",
        "if not Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists():\n",
        "    assert Path(f'train/_annotations.coco.json').exists()\n",
        "    assert Path(f'valid/_annotations.coco.json').exists()\n",
        "    assert Path(f'test/_annotations.coco.json').exists()\n",
        "    !mkdir -p $DATASET_PATH\n",
        "    !mv test train valid *txt $DATASET_PATH/\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWpUo2hk9u6Y"
      },
      "outputs": [],
      "source": [
        "assert Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists()\n",
        "assert Path(f'{DATASET_PATH}/valid/_annotations.coco.json').exists()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KelHWF79u6Z"
      },
      "source": [
        "# Training config file: nanodet-plus-m-1.5x_416-ppe.yml\n",
        "The following block of code creates the NanoDet training config file which\n",
        "is based on nanodet/config/nanodet-plus-m-1.5x_416.yml.\n",
        "Updated for the custom PPE dataset\n",
        "Change number of `total_epochs` for better performance.\n",
        "\n",
        "If training on GPU, then set   `gpu_ids`:\n",
        " * 1 gpu: [0]\n",
        " * 2 gpu: [0,1]\n",
        " * etc...\n",
        "\n",
        "Increase `total_epochs`, for example 20.\n",
        "\n",
        "Feel free to increase `val_intervals`, for example 10.\n",
        "\n",
        "For details see NanoDet github repo and [config docs](https://github.com/RangiLyu/nanodet/blob/main/docs/config_file_detail.md). Observe recommendation to adjust `lr` with `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAed6JDY9u6a"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "touch nanodet-plus-m-1.5x_416-ppe.yml\n",
        "cat <<EOF >nanodet-plus-m-1.5x_416-ppe.yml\n",
        "# Comments:\n",
        "# -  based on nanodet/config/nanodet-plus-m-1.5x_416.yml\n",
        "# -  \"device\": settings for colab T4 GPU\n",
        "# -  \"total_epochs\": set to 20 during testing, default 300\n",
        "save_dir: workspace/nanodet-plus-m-1.5x_416-ppe\n",
        "\n",
        "model:\n",
        "  weight_averager:\n",
        "    name: ExpMovingAverager\n",
        "    decay: 0.9998\n",
        "  arch:\n",
        "    name: NanoDetPlus\n",
        "    detach_epoch: 10\n",
        "    backbone:\n",
        "      name: ShuffleNetV2\n",
        "      model_size: 1.5x\n",
        "      out_stages: [2,3,4]\n",
        "      activation: LeakyReLU\n",
        "    fpn:\n",
        "      name: GhostPAN\n",
        "      in_channels: [176, 352, 704]\n",
        "      out_channels: 128\n",
        "      kernel_size: 5\n",
        "      num_extra_level: 1\n",
        "      use_depthwise: True\n",
        "      activation: LeakyReLU\n",
        "    head:\n",
        "      name: NanoDetPlusHead\n",
        "      num_classes: 1\n",
        "      input_channel: 128\n",
        "      feat_channels: 128\n",
        "      stacked_convs: 2\n",
        "      kernel_size: 5\n",
        "      strides: [8, 16, 32, 64]\n",
        "      activation: LeakyReLU\n",
        "      reg_max: 7\n",
        "      norm_cfg:\n",
        "        type: BN\n",
        "      loss:\n",
        "        loss_qfl:\n",
        "          name: QualityFocalLoss\n",
        "          use_sigmoid: True\n",
        "          beta: 2.0\n",
        "          loss_weight: 1.0\n",
        "        loss_dfl:\n",
        "          name: DistributionFocalLoss\n",
        "          loss_weight: 0.25\n",
        "        loss_bbox:\n",
        "          name: GIoULoss\n",
        "          loss_weight: 2.0\n",
        "    aux_head:\n",
        "      name: SimpleConvHead\n",
        "      num_classes: 1\n",
        "      input_channel: 256\n",
        "      feat_channels: 256\n",
        "      stacked_convs: 4\n",
        "      strides: [8, 16, 32, 64]\n",
        "      activation: LeakyReLU\n",
        "      reg_max: 7\n",
        "data:\n",
        "  train:\n",
        "    name: CocoDataset\n",
        "    img_path: /content/drive/MyDrive/dataset_blob/train\n",
        "    ann_path: /content/drive/MyDrive/dataset_blob/train/_annotations.coco.json\n",
        "    input_size: [716,716]\n",
        "    keep_ratio: False\n",
        "    pipeline:\n",
        "      perspective: 0.0\n",
        "      scale: [0.6, 1.4]\n",
        "      stretch: [[0.8, 1.2], [0.8, 1.2]]\n",
        "      rotation: 0\n",
        "      shear: 0\n",
        "      translate: 0.2\n",
        "      flip: 0.5\n",
        "      brightness: 0.2\n",
        "      contrast: [0.6, 1.4]\n",
        "      saturation: [0.5, 1.2]\n",
        "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
        "  val:\n",
        "    name: CocoDataset\n",
        "    img_path: /content/drive/MyDrive/dataset_blob/valid\n",
        "    ann_path: /content/drive/MyDrive/dataset_blob/valid/_annotations.coco.json\n",
        "    input_size: [716,716]\n",
        "    keep_ratio: False\n",
        "    pipeline:\n",
        "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
        "device:\n",
        "  gpu_ids: 1  # Use the first available GPU\n",
        "  workers_per_gpu: 2\n",
        "  batchsize_per_gpu: 32\n",
        "  precision: 32  # Change to 16 for AMP if needed\n",
        "schedule:\n",
        "  optimizer:\n",
        "    name: AdamW\n",
        "    lr: 0.001\n",
        "    weight_decay: 0.05\n",
        "  warmup:\n",
        "    name: linear\n",
        "    steps: 500\n",
        "    ratio: 0.0001\n",
        "  total_epochs: 30  # More epochs for better training\n",
        "  lr_schedule:\n",
        "    name: CosineAnnealingLR\n",
        "    T_max: 300\n",
        "    eta_min: 0.00005\n",
        "  val_intervals: 8\n",
        "grad_clip: 35\n",
        "evaluator:\n",
        "  name: CocoDetectionEvaluator\n",
        "  save_key: mAP\n",
        "log:\n",
        "  interval: 10\n",
        "\n",
        "class_names: [\n",
        "  'blob'\n",
        "  ]\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaP8y0Z19u6b"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2KpdZYL9u6c"
      },
      "outputs": [],
      "source": [
        "# OBSERVE: update the following assert statement to match your yml file settings.\n",
        "\n",
        "import yaml\n",
        "with open('nanodet-plus-m-1.5x_416-ppe.yml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "assert config['device']['gpu_ids'] == 1 or config['device']['gpu_ids'] == [0], print(f\"gpu_ids: {config['device']['gpu_ids']}\")\n",
        "assert config['schedule']['total_epochs'] == 2 or config['schedule']['total_epochs'] == 20, print(f\"total_epochs: {config['schedule']['total_epochs']}\")\n",
        "assert config['schedule']['val_intervals'] == 1 or config['schedule']['val_intervals'] == 10, print(f\"val_intervals: {config['schedule']['val_intervals']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==1.9.5 torchmetrics==0.11.4"
      ],
      "metadata": {
        "id": "pk9vKtdXXQQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers -y\n",
        "!pip install transformers==4.31.0"
      ],
      "metadata": {
        "id": "EjaWAgQEXndY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # This should return True if a GPU is available.\n",
        "print(torch.cuda.current_device())  # Check the current GPU device ID.\n",
        "print(torch.cuda.get_device_name(0))  # Check the name of the GPU."
      ],
      "metadata": {
        "id": "OpDeF7joZ1Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/nanodet/nanodet/evaluator/coco_detection.py"
      ],
      "metadata": {
        "id": "EWuZ20WwhefD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +w /content/nanodet/nanodet/evaluator/coco_detection.py"
      ],
      "metadata": {
        "id": "58D7V227hpfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Information points**\n",
        "\n",
        "1.   x_center, y_center, width, height\n",
        "2.   nanodet/evaluator/coco.detection.py\n",
        "3.   nanodet/tools/trainer.py\n",
        "4.   nanodet/data/collate.py --> string_classes = str remove the line\n",
        "5.   nanodet/model_using.yaml"
      ],
      "metadata": {
        "id": "75MwAyfK3d30"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bebrEoH9u6c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\"\n",
        "import torch\n",
        "assert '1.13' in torch.__version__, print(torch.__version__)\n",
        "assert Path('nanodet-plus-m-1.5x_416-ppe.yml').exists()\n",
        "!export PYTHONPATH=$PWD/nanodet:$PYTHONPATH && python nanodet/tools/train.py nanodet-plus-m-1.5x_416-ppe.yml\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "nanodet/nanodet/data/collate.py\n",
        "string_classes = str\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "print(f\"Using PyTorch version: {torch.__version__}\")\n",
        "\n",
        "from pathlib import Path\n",
        "assert Path('nanodet-plus-m-1.5x_416-ppe.yml').exists(), \"YAML config file not found!\"\n",
        "\n",
        "# Start training\n",
        "!export PYTHONPATH=$PWD/nanodet:$PYTHONPATH && python nanodet/tools/train.py nanodet-plus-m-1.5x_416-ppe.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoFpCwSV9u6c"
      },
      "source": [
        "# Remove aux layers that are only used during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B64qQqj29u6d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,\"./nanodet\")\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "from nanodet.model.arch import build_model\n",
        "from nanodet.util import cfg, load_config, Logger\n",
        "\n",
        "def remove_aux(cfg, model_path, remove_layers=['aux_fpn', 'aux_head'], debug=False):\n",
        "    model = build_model(cfg.model)\n",
        "    ckpt = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "    if len(remove_layers) > 0:\n",
        "        state_dict = copy.deepcopy(ckpt['state_dict'])\n",
        "        for rlayer in remove_layers:\n",
        "            for layer in ckpt['state_dict']:\n",
        "                if rlayer in layer:\n",
        "                    del state_dict[layer]\n",
        "                    if debug:\n",
        "                        print(f'removed layer: {layer}')\n",
        "        del ckpt['state_dict']\n",
        "        ckpt['state_dict'] = copy.deepcopy(state_dict)\n",
        "        del state_dict\n",
        "    return ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjcwbcGJ9u6d"
      },
      "outputs": [],
      "source": [
        "config_path = 'nanodet-plus-m-1.5x_416-ppe.yml'\n",
        "model_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth'\n",
        "dst_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best-removed-aux.pth'\n",
        "\n",
        "load_config(cfg, config_path)\n",
        "ckpt = remove_aux(cfg, model_path, ['aux_fpn', 'aux_head'])\n",
        "torch.save(ckpt, dst_path)\n",
        "print(f'Saved to: {dst_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtcPqvf_9u6d"
      },
      "outputs": [],
      "source": [
        "# Compare size w and w/o aux\n",
        "!ls -l workspace/nanodet-plus-m-1.5x_416-ppe/model_best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jZOAxPL9u6d"
      },
      "source": [
        "# Quantization of custom NanoDet model using Model Compression Toolkit\n",
        "Quantization is based on https://github.com/sony/model_optimization/blob/v2.0.0/tutorials/notebooks/keras/ptq/example_keras_nanodet_plus.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA4paxhn9u6e"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBgkLJU49u6e"
      },
      "outputs": [],
      "source": [
        "!pip install --no-cache-dir -q \"tensorflow>=2.14.0,<2.15.0\" pycocotools\n",
        "import sys\n",
        "MCT_COMMIT = 'v2.1.0'\n",
        "!rm -rf local_mct\n",
        "!git clone https://github.com/sony/model_optimization.git local_mct && cd local_mct && git checkout {MCT_COMMIT} && pip install --no-cache-dir -r requirements.txt\n",
        "sys.path.insert(0,\"./local_mct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2pQb_qy9u6e"
      },
      "source": [
        "# Keras NanoDet float model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUiqtJL69u6f"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "assert '2.14' in tf.__version__, print(tf.__version__)\n",
        "assert '1.13' in torch.__version__, print(torch.__version__)\n",
        "\n",
        "from keras.models import Model\n",
        "import model_compression_toolkit as mct\n",
        "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_plus_m\n",
        "from tutorials.mct_model_garden.models_keras.utils.torch2keras_weights_translation import load_state_dict\n",
        "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_box_decoding\n",
        "assert 'local_mct' in mct.__file__, print(mct.__file__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMd_BFxP9u6f"
      },
      "outputs": [],
      "source": [
        "# Upload the trained custom model\n",
        "CUSTOM_WEIGHTS_FILE = dst_path  # The NanoDet model trained with PPE dataset\n",
        "CLASS_NAMES = [\n",
        "  'safety-equipment',\n",
        "  'person',\n",
        "  'goggles',\n",
        "  'helmet',\n",
        "  'no-goggles',\n",
        "  'no-helmet',\n",
        "  'no-vest',\n",
        "  'vest']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "DATASET_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train'\n",
        "ANNOT_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train/_annotations.coco.json'\n",
        "DATASET_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid'\n",
        "ANNOT_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/_annotations.coco.json'\n",
        "DATASET_REPR = DATASET_VALID\n",
        "ANNOT_REPR = ANNOT_VALID\n",
        "\n",
        "QUANTIZED_MODEL_NAME = 'nanodet-quant-ppe.keras'\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "N_ITER = 20  # 1 for testing, otherwise 20\n",
        "\n",
        "assert Path(CUSTOM_WEIGHTS_FILE).exists()\n",
        "assert Path(DATASET_REPR).exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR9tNaN39u6g"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_model(weights=CUSTOM_WEIGHTS_FILE, num_classes=NUM_CLASSES):\n",
        "    INPUT_RESOLUTION = 416\n",
        "    INPUT_SHAPE = (INPUT_RESOLUTION, INPUT_RESOLUTION, 3)\n",
        "    SCALE_FACTOR = 1.5\n",
        "    BOTTLENECK_RATIO = 0.5\n",
        "    FEATURE_CHANNELS = 128\n",
        "\n",
        "    pretrained_weights = torch.load(weights, map_location=torch.device('cpu'))['state_dict']\n",
        "    # Generate Nanodet base model\n",
        "    model = nanodet_plus_m(INPUT_SHAPE, SCALE_FACTOR, BOTTLENECK_RATIO, FEATURE_CHANNELS, num_classes)\n",
        "\n",
        "    # Set the pre-trained weights\n",
        "    load_state_dict(model, state_dict_torch=pretrained_weights)\n",
        "\n",
        "    # Add Nanodet Box decoding layer (decode the model outputs to bounding box coordinates)\n",
        "    scores, boxes = nanodet_box_decoding(model.output, res=INPUT_RESOLUTION, num_classes=num_classes)\n",
        "\n",
        "    # Add TensorFlow NMS layer\n",
        "    outputs = tf.image.combined_non_max_suppression(\n",
        "        boxes,\n",
        "        scores,\n",
        "        max_output_size_per_class=300,\n",
        "        max_total_size=300,\n",
        "        iou_threshold=0.65,\n",
        "        score_threshold=0.001,\n",
        "        pad_per_class=False,\n",
        "        clip_boxes=False\n",
        "        )\n",
        "\n",
        "    model = Model(model.input, outputs, name='Nanodet_plus_m_1.5x_416')\n",
        "\n",
        "    print('Model is ready for evaluation')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4ij5P9m9u6g"
      },
      "outputs": [],
      "source": [
        "# known warning:  WARNING: head.distribution_project.project not assigned to keras model !!!\n",
        "float_model = get_model(CUSTOM_WEIGHTS_FILE, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QgLmp2U9u6h"
      },
      "source": [
        "# PTQ quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d06kjCOy9u6h"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Iterator, Tuple, List\n",
        "\n",
        "import cv2\n",
        "from tutorials.mct_model_garden.evaluation_metrics.coco_evaluation import coco_dataset_generator, CocoEval\n",
        "\n",
        "def nanodet_preprocess(x):\n",
        "    img_mean = [103.53, 116.28, 123.675]\n",
        "    img_std = [57.375, 57.12, 58.395]\n",
        "    x = cv2.resize(x, (416, 416))\n",
        "    x = (x - img_mean) / img_std\n",
        "    return x\n",
        "\n",
        "def get_representative_dataset(n_iter: int, dataset_loader: Iterator[Tuple]):\n",
        "    def representative_dataset() -> Iterator[List]:\n",
        "        ds_iter = iter(dataset_loader)\n",
        "        for _ in range(n_iter):\n",
        "            yield [next(ds_iter)[0]]\n",
        "\n",
        "    return representative_dataset\n",
        "\n",
        "def quantization(float_model, dataset, annot, n_iter=N_ITER):\n",
        "    # Load representative dataset\n",
        "    representative_dataset = coco_dataset_generator(dataset_folder=dataset,\n",
        "                                                    annotation_file=annot,\n",
        "                                                    preprocess=nanodet_preprocess,\n",
        "                                                    batch_size=BATCH_SIZE)\n",
        "\n",
        "    tpc = mct.get_target_platform_capabilities('tensorflow', 'imx500')\n",
        "\n",
        "    # Preform post training quantization\n",
        "    quant_model, _ = mct.ptq.keras_post_training_quantization(\n",
        "        float_model,\n",
        "        representative_data_gen=get_representative_dataset(n_iter, representative_dataset),\n",
        "        target_platform_capabilities=tpc)\n",
        "\n",
        "    print('Quantized model is ready')\n",
        "    return quant_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzMJ6VZQ9u6h"
      },
      "outputs": [],
      "source": [
        "quant_model = quantization(float_model, DATASET_REPR, ANNOT_REPR)\n",
        "print(f'Representative dataset: {DATASET_REPR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19HuzFWI9u6h"
      },
      "outputs": [],
      "source": [
        "# Observe that loading quantized model might require specification of custom layers,\n",
        "# see https://github.com/sony/model_optimization/issues/1104\n",
        "quant_model.save(QUANTIZED_MODEL_NAME)\n",
        "print(f'Quantized model saved: {QUANTIZED_MODEL_NAME}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxulb2gm9u6i"
      },
      "source": [
        "_todo_: coco evaluation of the custom quantized NanoDet model requires some update to the mct repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zgZ5vKY9u6i"
      },
      "source": [
        "# Visualize detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-qvobZU9u6j"
      },
      "outputs": [],
      "source": [
        "# Helper functions for visualization\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_image(image_path: str, preprocess: Callable) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load and preprocess an image from a given file path.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "        preprocess (function): Preprocessing function to apply to the loaded image.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Preprocessed image.\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image = preprocess(image)\n",
        "    image = np.expand_dims(image, 0)\n",
        "    return image\n",
        "\n",
        "# draw a single bounding box onto a numpy array image\n",
        "def draw_bounding_box(img, annotation, scale, class_id, score):\n",
        "    row = scale[0]\n",
        "    col = scale[1]\n",
        "    x_min, y_min = int(annotation[1]*col), int(annotation[0]*row)\n",
        "    x_max, y_max = int(annotation[3]*col), int(annotation[2]*row)\n",
        "\n",
        "    color = (0,255,0)\n",
        "\n",
        "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
        "    text = f'{int(class_id)}: {score:.2f}'\n",
        "    cv2.putText(img, text, (x_min + 10, y_min + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "# draw all annotation bounding boxes on an image\n",
        "def annotate_image(img, output, scale, quantized_model=False, threshold=0.55):\n",
        "    if quantized_model:\n",
        "        b = output[0].numpy()[0]\n",
        "        s = output[1].numpy()[0]\n",
        "        c = output[2].numpy()[0]\n",
        "    else:\n",
        "        print('Assuming float model')\n",
        "        b = output.nmsed_boxes.numpy()[0]\n",
        "        s = output.nmsed_scores.numpy()[0]\n",
        "        c = output.nmsed_classes.numpy()[0]\n",
        "    for index, row in enumerate(b):\n",
        "        if s[index] > threshold:\n",
        "            #print(f'row: {row}')\n",
        "            id = int(c[index])\n",
        "            draw_bounding_box(img, row, scale, id, s[index])\n",
        "            print(f'class: {CLASS_NAMES[id]} ({id}), score: {s[index]:.2f}')\n",
        "    return {'bbox':b, 'score':s, 'classes':c}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03PI8t6U9u6j"
      },
      "outputs": [],
      "source": [
        "# See appendix for results. For 2 epochs, the bounding boxes are not perfect...\n",
        "# But improves considerably for 20 epochs.\n",
        "test_img = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/image_257_jpg.rf.1a3a6eb456134cce302712c109645c26.jpg'\n",
        "img = load_and_preprocess_image(f'{test_img}', nanodet_preprocess)\n",
        "output = quant_model(img)\n",
        "image = cv2.imread(f'{test_img}')\n",
        "print(f'image shape: {image.shape}')\n",
        "r = annotate_image(image, output, scale=image.shape, quantized_model=True)\n",
        "assert r['score'][0] > 0.5, print(f\"r['score'][0] > 0.5 failed: {r['score'][0]}\")\n",
        "dst = f'annotated.jpg'\n",
        "if cv2.imwrite(dst, image):\n",
        "    print(f'Annotated image saved to: {dst}')\n",
        "else:\n",
        "    print(f'Failed saving annotated image')\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZBs1_Vb9u6k"
      },
      "source": [
        "# Next step\n",
        "__OBSERVE__: First, save the quantized model to your local machine. You will need it for the conversion and packaging steps.\n",
        "\n",
        "Next step is to convert and package the model for IMX500. _todo: link to further instructions. This model requires bgr settings in the post-converter._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kfNfWTT9u6k"
      },
      "source": [
        "# Appendix\n",
        "## Results total_epochs=2\n",
        "```\n",
        "[NanoDet][07-12 10:47:40]INFO:\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.191\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.026\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.011\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.045\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.076\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.100\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.207\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.224\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.149\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.271\n",
        "\n",
        "[NanoDet][07-12 10:47:40]INFO:\n",
        "| class            | AP50   | mAP   | class     | AP50   | mAP   |\n",
        "|:-----------------|:-------|:------|:----------|:-------|:------|\n",
        "| Safety-Equipment | nan    | nan   | Person    | 54.1   | 20.1  |\n",
        "| goggles          | 1.6    | 0.4   | helmet    | 40.3   | 13.4  |\n",
        "| no-goggles       | 6.9    | 2.3   | no-helmet | 0.0    | 0.0   |\n",
        "| no-vest          | 3.9    | 1.0   | vest      | 27.1   | 7.9   |\n",
        "[NanoDet][07-12 10:47:40]INFO:Saving model to workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth\n",
        "[NanoDet][07-12 10:47:40]INFO:Val_metrics: {'mAP': 0.06442128900684024, 'AP_50': 0.1912998265318579, 'AP_75': 0.026441697602184976, 'AP_small': 0.010583883892274994, 'AP_m': 0.044976540581496194, 'AP_l': 0.0756733533889817}\n",
        "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
        "```\n",
        "## Results total_epochs=20\n",
        "```\n",
        "Comments:\n",
        "-  based on nanodet/config/nanodet-plus-m-1.5x_416.yml\n",
        "-  \"device\": settings for colab T4 GPU\n",
        "-  \"total_epochs\": set to 20 during testing, default 300\n",
        "...\n",
        "[NanoDet][05-16 09:25:43]INFO:\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.301\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.610\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.250\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.078\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.362\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.281\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.479\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.497\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.185\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.414\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.570\n",
        "\n",
        "[NanoDet][05-16 09:25:43]INFO:\n",
        "| class            | AP50   | mAP   | class     | AP50   | mAP   |\n",
        "|:-----------------|:-------|:------|:----------|:-------|:------|\n",
        "| Safety-Equipment | nan    | nan   | Person    | 86.4   | 49.3  |\n",
        "| goggles          | 35.7   | 15.4  | helmet    | 86.0   | 46.3  |\n",
        "| no-goggles       | 42.6   | 16.6  | no-helmet | 34.2   | 13.8  |\n",
        "| no-vest          | 59.6   | 26.3  | vest      | 82.5   | 42.8  |\n",
        "[NanoDet][05-16 09:25:44]INFO:Saving model to workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth\n",
        "[NanoDet][05-16 09:25:44]INFO:Val_metrics: {'mAP': 0.3006027561087712, 'AP_50': 0.6099170448933922, 'AP_75': 0.2496291506747232, 'AP_small': 0.07788513248169772, 'AP_m': 0.212229159695, 'AP_l': 0.36198435595574324}\n",
        "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
        "\n",
        "real\t21m35.722s\n",
        "user\t25m48.699s\n",
        "sys\t6m6.849s\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QY3_VmQC9PL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "\n",
        "def detect_rbc_dots(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    params = cv2.SimpleBlobDetector_Params()\n",
        "    params.filterByColor = True\n",
        "    params.blobColor = 0  # Detect dark blobs\n",
        "    params.minArea = 5\n",
        "    params.maxArea = 500\n",
        "    params.filterByCircularity = False\n",
        "    detector = cv2.SimpleBlobDetector_create(params)\n",
        "\n",
        "    keypoints = detector.detect(img)\n",
        "\n",
        "    bboxes = []\n",
        "    for kp in keypoints:\n",
        "        x = int(kp.pt[0] - kp.size / 2)\n",
        "        y = int(kp.pt[1] - kp.size / 2)\n",
        "        w = int(kp.size)\n",
        "        h = int(kp.size)\n",
        "        bboxes.append((x, y, w, h))\n",
        "\n",
        "    return bboxes\n",
        "\n",
        "# Example usage\n",
        "frame_path = '/content/image_0000.jpg'\n",
        "bboxes = detect_rbc_dots(frame_path)\n",
        "\n",
        "# Draw and save preview\n",
        "img = cv2.imread(frame_path)\n",
        "for x, y, w, h in bboxes:\n",
        "    cv2.rectangle(img, (x, y), (x+w, y+h), (0,255,0), 1)\n",
        "cv2.imwrite('preview_detected.jpg', img)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/dataset_blob/val'  # change this to the actual path\n",
        "all_files = os.listdir(input_folder)\n",
        "\n",
        "image_files = [f for f in all_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "\n",
        "print(f\"Total files in folder: {len(all_files)}\")\n",
        "print(f\"Image files detected: {len(image_files)}\")"
      ],
      "metadata": {
        "id": "EX4_AuR1utvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNvFnNfQIGbj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Folder paths\n",
        "input_folder = '/content/croped_test_blob_folder'\n",
        "output_folder = '/content/test'\n",
        "annotation_file = '/content/test_annotations.coco.json'\n",
        "\n",
        "# Create output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Blob detector settings\n",
        "params = cv2.SimpleBlobDetector_Params()\n",
        "params.filterByColor = True\n",
        "params.blobColor = 0\n",
        "params.minThreshold = 10\n",
        "params.maxThreshold = 200\n",
        "\n",
        "params.filterByArea = True\n",
        "params.minArea = 5\n",
        "params.maxArea = 50\n",
        "\n",
        "params.filterByCircularity = True\n",
        "params.minCircularity = 0.7\n",
        "\n",
        "params.filterByInertia = False\n",
        "params.filterByConvexity = False\n",
        "\n",
        "# COCO format JSON structure\n",
        "coco = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": [\n",
        "        {\"id\": 1, \"name\": \"blob\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "image_id = 1\n",
        "annotation_id = 1\n",
        "\n",
        "# Iterate through all images in the input folder\n",
        "for image_name in tqdm(os.listdir(input_folder)):\n",
        "    if image_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "        original_img_path = os.path.join(input_folder, image_name)\n",
        "        original_img = cv2.imread(original_img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if original_img is None:\n",
        "            continue\n",
        "\n",
        "        original_height, original_width = original_img.shape\n",
        "        resized_width, resized_height = 716, 716\n",
        "        scale_x = resized_width / original_width\n",
        "        scale_y = resized_height / original_height\n",
        "\n",
        "        detector = cv2.SimpleBlobDetector_create(params)\n",
        "        keypoints = detector.detect(original_img)\n",
        "\n",
        "        resized_img = cv2.resize(original_img, (resized_width, resized_height))\n",
        "        save_path = os.path.join(output_folder, image_name)\n",
        "        cv2.imwrite(save_path, resized_img)\n",
        "\n",
        "        # Add image entry to COCO\n",
        "        coco[\"images\"].append({\n",
        "            \"id\": image_id,\n",
        "            \"file_name\": image_name,\n",
        "            \"width\": resized_width,\n",
        "            \"height\": resized_height\n",
        "        })\n",
        "\n",
        "        for kp in keypoints:\n",
        "            x = kp.pt[0] * scale_x\n",
        "            y = kp.pt[1] * scale_y\n",
        "            r = (kp.size / 2.0) * ((scale_x + scale_y) / 2.0)\n",
        "            width = 2 * r\n",
        "            height = 2 * r\n",
        "            x_min = x - r\n",
        "            y_min = y - r\n",
        "            x_max = x + r\n",
        "            y_max = y + r\n",
        "\n",
        "            # Skip invalid or near-zero area boxes\n",
        "            if width <= 0.5 or height <= 0.5:\n",
        "                continue\n",
        "\n",
        "            coco[\"annotations\"].append({\n",
        "                \"id\": annotation_id,\n",
        "                \"image_id\": image_id,\n",
        "                \"category_id\": 1,\n",
        "                \"bbox\": [round(x_min, 2), round(y_min, 2), round(x_max, 2), round(y_max, 2)],\n",
        "                \"area\": round(width * height, 2),\n",
        "                \"iscrowd\": 0,\n",
        "                \"segmentation\": [[\n",
        "                    round(x + r, 2), round(y, 2),\n",
        "                    round(x, 2), round(y + r, 2),\n",
        "                    round(x - r, 2), round(y, 2),\n",
        "                    round(x, 2), round(y - r, 2)\n",
        "                ]]\n",
        "            })\n",
        "            annotation_id += 1\n",
        "\n",
        "# Save COCO JSON\n",
        "with open(annotation_file, 'w') as f:\n",
        "    json.dump(coco, f, indent=4)\n",
        "\n",
        "print(f\"Saved annotations to {annotation_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yllwme4TC9Tp"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load grayscale image\n",
        "img = cv2.imread('/content/image_1083.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Blob detector settings\n",
        "params = cv2.SimpleBlobDetector_Params()\n",
        "params.filterByColor = True\n",
        "params.blobColor = 0  # Detect dark blobs\n",
        "params.minThreshold = 10\n",
        "params.maxThreshold = 200\n",
        "\n",
        "params.filterByArea = True\n",
        "params.minArea = 5     # Adjust for your dots\n",
        "params.maxArea = 50    # Exclude large blobs\n",
        "\n",
        "params.filterByCircularity = True\n",
        "params.minCircularity = 0.7  # Helps ignore irregular shapes\n",
        "\n",
        "params.filterByInertia = False\n",
        "params.filterByConvexity = False\n",
        "\n",
        "# Create detector and detect\n",
        "detector = cv2.SimpleBlobDetector_create(params)\n",
        "keypoints = detector.detect(img)\n",
        "\n",
        "# Draw detections\n",
        "img_with_keypoints = cv2.drawKeypoints(\n",
        "    img, keypoints, None, (0, 255, 0),\n",
        "    cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
        ")\n",
        "\n",
        "# Show results\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(img_with_keypoints, cmap='gray')\n",
        "plt.title(f\"Detected dots: {len(keypoints)}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE12beBMGwqA"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load grayscale image\n",
        "img = cv2.imread('/content/image_0655.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Blob detector settings\n",
        "params = cv2.SimpleBlobDetector_Params()\n",
        "params.filterByColor = True\n",
        "params.blobColor = 0  # Detect dark blobs\n",
        "params.minThreshold = 10\n",
        "params.maxThreshold = 200\n",
        "\n",
        "params.filterByArea = True\n",
        "params.minArea = 5\n",
        "params.maxArea = 50\n",
        "\n",
        "params.filterByCircularity = True\n",
        "params.minCircularity = 0.7\n",
        "\n",
        "params.filterByInertia = False\n",
        "params.filterByConvexity = False\n",
        "\n",
        "# Create detector and detect\n",
        "detector = cv2.SimpleBlobDetector_create(params)\n",
        "keypoints = detector.detect(img)\n",
        "\n",
        "# Draw detections\n",
        "img_with_keypoints = cv2.drawKeypoints(\n",
        "    img, keypoints, None, (0, 255, 0),\n",
        "    cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
        ")\n",
        "\n",
        "# Print bounding box coordinates for each detected blob\n",
        "print(\"Bounding box coordinates (xmin, ymin, xmax, ymax):\")\n",
        "for i, kp in enumerate(keypoints):\n",
        "    x = kp.pt[0]  # x-coordinate of the center\n",
        "    y = kp.pt[1]  # y-coordinate of the center\n",
        "    r = kp.size / 2  # radius\n",
        "\n",
        "    xmin = int(x - r)\n",
        "    ymin = int(y - r)\n",
        "    xmax = int(x + r)\n",
        "    ymax = int(y + r)\n",
        "\n",
        "    print(f\"Dot {i + 1}: xmin={xmin}, ymin={ymin}, xmax={xmax}, ymax={ymax}\")\n",
        "\n",
        "# Show results\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(img_with_keypoints, cmap='gray')\n",
        "plt.title(f\"Detected dots: {len(keypoints)}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDpLp5eszXbI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# ==== SETTINGS ====\n",
        "input_folder = '/content/drive/MyDrive/blob_frames1'           # Change to your input folder\n",
        "output_folder = '/content/drive/MyDrive/croped_blob_folder_1080'          # Folder to save processed images\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Crop ratios from left and right (e.g., 0.1 = crop 10% of width)\n",
        "crop_ratio_left = 0.257\n",
        "crop_ratio_right = 0.244\n",
        "\n",
        "# Final resize size\n",
        "final_size = (1080, 1080)\n",
        "\n",
        "# ==== PROCESS ====\n",
        "image_counter = 1  # Start naming from image_0001\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Failed to load image: {filename}\")\n",
        "            continue\n",
        "\n",
        "        height, width = img.shape[:2]\n",
        "\n",
        "        # Compute crop boundaries\n",
        "        left = int(crop_ratio_left * width)\n",
        "        right = int(width - crop_ratio_right * width)\n",
        "\n",
        "        # Crop image from left and right\n",
        "        cropped_img = img[:, left:right]\n",
        "\n",
        "        # Resize to final size\n",
        "        resized_img = cv2.resize(cropped_img, final_size)\n",
        "\n",
        "        # New sequential filename\n",
        "        new_filename = f\"image_{image_counter:04d}.jpg\"\n",
        "        output_path = os.path.join(output_folder, new_filename)\n",
        "        cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "        print(f\"Processed and saved: {new_filename}\")\n",
        "        image_counter += 1\n",
        "\n",
        "print(f\"\\nAll images saved to: {output_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qjt-1SFCvhY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "dataset/\n",
        "â”œâ”€â”€ images/\n",
        "â”‚   â”œâ”€â”€ train/\n",
        "â”‚   â””â”€â”€ val/\n",
        "â”œâ”€â”€ annotations/\n",
        "â”‚   â”œâ”€â”€ train.json\n",
        "â”‚   â””â”€â”€ val.json\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2M1NwlzCvk8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "10 frames with ~200 dots.\n",
        "\n",
        "10 frames with ~300+ dots.\n",
        "\n",
        "10+ frames with occlusion, overlapping dots, or motion blur.\n",
        "\n",
        "Ensure different lighting/backgrounds if present in test videos.\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}